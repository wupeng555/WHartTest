
"""Orchestratoræµå¼å¯¹è¯æ¥å£"""
import json
import logging
import uuid
import asyncio
import os
from django.views import View
from django.http import StreamingHttpResponse
from django.utils.decorators import method_decorator
from django.views.decorators.csrf import csrf_exempt
from django.conf import settings
from rest_framework_simplejwt.authentication import JWTAuthentication
from rest_framework.exceptions import AuthenticationFailed
from rest_framework import viewsets, status
from rest_framework.permissions import IsAuthenticated
from asgiref.sync import sync_to_async

from langgraph_integration.models import LLMConfig, ChatSession
from projects.models import Project, ProjectMember
from prompts.models import UserPrompt, PromptType
from .models import OrchestratorTask
from .serializers import OrchestratorTaskSerializer
from .graph import create_orchestrator_graph, OrchestratorState
from .context_compression import CompressionSettings
from langgraph_integration.views import create_llm_instance, create_sse_data
from wharttest_django.checkpointer import get_async_checkpointer

logger = logging.getLogger(__name__)


class OrchestratorTaskViewSet(viewsets.ReadOnlyModelViewSet):
    """åªè¯»è§†å›¾ - ç”¨äºæŸ¥çœ‹å†å²ä»»åŠ¡è®°å½•"""
    permission_classes = [IsAuthenticated]
    queryset = OrchestratorTask.objects.all()
    serializer_class = OrchestratorTaskSerializer
    
    def get_queryset(self):
        """åªè¿”å›å½“å‰ç”¨æˆ·çš„ä»»åŠ¡"""
        return OrchestratorTask.objects.filter(user=self.request.user)


@method_decorator(csrf_exempt, name='dispatch')
class OrchestratorStreamAPIView(View):
    """
    Orchestratoræµå¼å¯¹è¯æ¥å£
    
    Brainé€šè¿‡StateGraphè°ƒç”¨å„ä¸ªAgent,æ‰€æœ‰äº¤äº’ä»¥SSEæµå¼è¿”å›:
    - Brainçš„å†³ç­–è¿‡ç¨‹
    - Requirement Agentçš„éœ€æ±‚åˆ†æ
    - Knowledge Agentçš„æ–‡æ¡£æ£€ç´¢
    - TestCase Agentçš„ç”¨ä¾‹ç”Ÿæˆ
    
    å¯¹è¯å†å²è‡ªåŠ¨ä¿å­˜åˆ°chat_history.sqlite,å¯é€šè¿‡langgraphçš„å†å²æ¥å£æŸ¥è¯¢
    """
    
    async def authenticate_request(self, request):
        """æ‰‹åŠ¨è¿›è¡ŒJWTè®¤è¯ï¼ˆå¼‚æ­¥ç‰ˆæœ¬ï¼‰"""
        auth_header = request.META.get('HTTP_AUTHORIZATION')
        if not auth_header or not auth_header.startswith('Bearer '):
            raise AuthenticationFailed('Authentication credentials were not provided.')
        
        token = auth_header.split(' ')[1]
        jwt_auth = JWTAuthentication()
        
        try:
            validated_token = await sync_to_async(jwt_auth.get_validated_token)(token)
            user = await sync_to_async(jwt_auth.get_user)(validated_token)
            return user
        except Exception as e:
            raise AuthenticationFailed(f'Invalid token: {str(e)}')
    
    def _check_project_permission(self, user, project_id):
        """æ£€æŸ¥ç”¨æˆ·æ˜¯å¦æœ‰è®¿é—®æŒ‡å®šé¡¹ç›®çš„æƒé™"""
        try:
            project = Project.objects.get(id=project_id)
            if user.is_superuser:
                return project
            if ProjectMember.objects.filter(project=project, user=user).exists():
                return project
            return None
        except Project.DoesNotExist:
            return None
    
    async def _create_sse_generator(self, request, user_message_content, session_id, 
                                   project_id, project, prompt_id=None):
        """åˆ›å»ºSSEæ•°æ®ç”Ÿæˆå™¨,é›†æˆStateGraph with checkpointer"""
        try:
            # 1. è·å–æ´»è·ƒçš„LLMé…ç½®
            active_config = await sync_to_async(LLMConfig.objects.get)(is_active=True)
            logger.info(f"OrchestratorStream: Using LLM: {active_config.name}")
        except LLMConfig.DoesNotExist:
            yield create_sse_data({'type': 'error', 'message': 'No active LLM configuration found'})
            return
        except LLMConfig.MultipleObjectsReturned:
            yield create_sse_data({'type': 'error', 'message': 'Multiple active LLM configurations found'})
            return
        
        try:
            # 2. åˆ›å»ºLLMå®ä¾‹
            llm = create_llm_instance(active_config, temperature=0.7)
            logger.info(f"OrchestratorStream: LLM initialized")
            
            # 2.1 åˆ›å»ºä¸Šä¸‹æ–‡å‹ç¼©é…ç½®
            compression_settings = CompressionSettings(
                max_context_tokens=active_config.context_limit,
                trigger_ratio=getattr(settings, "ORCHESTRATOR_CONTEXT_TRIGGER_RATIO", 0.6),
                preserve_recent_messages=getattr(settings, "ORCHESTRATOR_PRESERVE_RECENT_MESSAGES", 8),
            )
            logger.info(f"OrchestratorStream: Context compression configured (limit={active_config.context_limit})")
            
            # 2.5 åŠ è½½MCPå·¥å…·ï¼ˆä¸ChatStreamAPIViewä¸€è‡´ï¼‰
            mcp_tools_list = []
            try:
                from mcp_tools.models import RemoteMCPConfig
                from mcp_tools.persistent_client import mcp_session_manager
                
                # è¯Šæ–­ï¼šæ£€æŸ¥RemoteMCPConfigé…ç½®
                total_configs = await sync_to_async(RemoteMCPConfig.objects.count)()
                logger.info(f"OrchestratorStream: æ•°æ®åº“ä¸­å…±æœ‰ {total_configs} ä¸ªMCPé…ç½®")
                
                active_remote_mcp_configs_qs = RemoteMCPConfig.objects.filter(is_active=True)
                active_remote_mcp_configs = await sync_to_async(list)(active_remote_mcp_configs_qs)
                logger.info(f"OrchestratorStream: æ¿€æ´»çš„MCPé…ç½®æ•°é‡: {len(active_remote_mcp_configs)}")
                
                if active_remote_mcp_configs:
                    client_mcp_config = {}
                    for r_config in active_remote_mcp_configs:
                        config_key = r_config.name or f"remote_config_{r_config.id}"
                        client_mcp_config[config_key] = {
                            "url": r_config.url,
                            "transport": (r_config.transport or "streamable_http").replace('-', '_'),
                        }
                        if r_config.headers and isinstance(r_config.headers, dict) and r_config.headers:
                            client_mcp_config[config_key]["headers"] = r_config.headers
                    
                    if client_mcp_config:
                        logger.info(f"OrchestratorStream: åŠ è½½MCPé…ç½®: {list(client_mcp_config.keys())}")
                        mcp_tools_list = await mcp_session_manager.get_tools_for_config(
                            client_mcp_config,
                            user_id=str(request.user.id),
                            project_id=str(project_id),
                            session_id=session_id
                        )
                        logger.info(f"âœ… OrchestratorStream: æˆåŠŸåŠ è½½ {len(mcp_tools_list)} ä¸ªMCPå·¥å…·")
                    else:
                        logger.warning("âš ï¸ OrchestratorStream: æ— å¯ç”¨çš„MCPé…ç½® - æ‰€æœ‰agentå°†æ— æ³•ä½¿ç”¨MCPå·¥å…·")
                else:
                    logger.warning(f"âš ï¸ OrchestratorStream: æœªæ‰¾åˆ°æ¿€æ´»çš„RemoteMCPConfigï¼ˆå…±{len(active_remote_mcp_configs)}ä¸ªé…ç½®ï¼Œä½†æ— æ¿€æ´»çŠ¶æ€ï¼‰")
            except Exception as e:
                logger.error(f"âŒ OrchestratorStream: åŠ è½½MCPå·¥å…·å¤±è´¥: {e}", exc_info=True)
                logger.error("   å»ºè®®æ£€æŸ¥ï¼š1) RemoteMCPConfigé…ç½® 2) MCPæœåŠ¡è¿æ¥ 3) mcp_session_managerçŠ¶æ€")
                # mcp_tools_listä¿æŒä¸ºç©ºï¼Œç»§ç»­æ‰§è¡Œ
            
            # 3. è·å–Brainçš„ç³»ç»Ÿæç¤ºè¯
            brain_prompt = None
            try:
                if prompt_id:
                    user_prompt = await sync_to_async(UserPrompt.objects.get)(
                        id=prompt_id,
                        user=request.user,
                        prompt_type=PromptType.BRAIN_ORCHESTRATOR,
                        is_active=True
                    )
                    brain_prompt = user_prompt.content
                    logger.info(f"OrchestratorStream: Using user-specified Brain prompt")
                else:
                    user_prompt = await sync_to_async(
                        lambda: UserPrompt.objects.filter(
                            user=request.user,
                            prompt_type=PromptType.BRAIN_ORCHESTRATOR,
                            is_active=True
                        ).first()
                    )()
                    if user_prompt:
                        brain_prompt = user_prompt.content
                        logger.info(f"OrchestratorStream: Using user's Brain prompt")
            except Exception as e:
                logger.warning(f"OrchestratorStream: Failed to get user Brain prompt: {e}")
            
            if not brain_prompt:
                from .prompts import BRAIN_AGENT_PROMPT
                brain_prompt = BRAIN_AGENT_PROMPT
                logger.info(f"OrchestratorStream: Using default Brain prompt")
            
            # 4. åˆ›å»ºcheckpointerå’ŒStateGraphï¼ˆä¼ é€’MCPå·¥å…·å’Œproject_idï¼‰
            async with get_async_checkpointer() as checkpointer:
                graph = create_orchestrator_graph(
                    llm,
                    checkpointer,
                    user=request.user,
                    mcp_tools=mcp_tools_list,
                    project_id=project_id,
                    compression_settings=compression_settings,
                    model_name=active_config.name
                )
                logger.info(f"OrchestratorStream: StateGraph created with {len(mcp_tools_list)} MCP tools, project_id={project_id}, user={request.user.username}")
                if len(mcp_tools_list) == 0:
                    logger.warning("âš ï¸ è­¦å‘Šï¼šå½“å‰æ²¡æœ‰å¯ç”¨çš„MCPå·¥å…·ï¼æ‰€æœ‰agentå°†åªèƒ½ä½¿ç”¨çŸ¥è¯†åº“å·¥å…·ã€‚")
                    logger.warning("   è¯·æ£€æŸ¥RemoteMCPConfigè¡¨ä¸­æ˜¯å¦æœ‰is_active=Trueçš„è®°å½•ã€‚")
                
                # 5. æ„å»ºthread_idï¼ˆåŒ…å«é¡¹ç›®IDå®ç°é¡¹ç›®éš”ç¦»ï¼‰
                thread_id_parts = [str(request.user.id), str(project_id)]
                if session_id:
                    thread_id_parts.append(str(session_id))
                thread_id = "_".join(thread_id_parts)
                logger.info(f"OrchestratorStream: Using thread_id: {thread_id}")
                
                # 6. æ„å»ºè¾“å…¥çŠ¶æ€ï¼ˆåªåŒ…å«æ–°æ¶ˆæ¯ï¼Œcheckpointerä¼šè‡ªåŠ¨åˆå¹¶å†å²ï¼‰
                from langchain_core.messages import HumanMessage
                
                # å…³é”®ï¼šåªä¼ å…¥æ–°æ¶ˆæ¯å’Œå¿…è¦çš„åˆå§‹å­—æ®µ
                # checkpointerä¼šè‡ªåŠ¨åŠ è½½å†å²messageså¹¶è¿½åŠ æ–°æ¶ˆæ¯
                input_state: OrchestratorState = {
                    "messages": [HumanMessage(content=user_message_content)],
                    "requirement": user_message_content,
                    "project_id": project_id,
                    "requirement_analysis": None,
                    "knowledge_docs": [],
                    "testcases": [],
                    "next_agent": "",
                    "instruction": "",
                    "reason": "",
                    "current_step": 0,
                    "max_steps": 10,
                    "context_summary": None,
                    "summarized_message_count": 0,
                    "context_token_count": 0
                }
                
                # 7. å‘é€å¼€å§‹ä¿¡å·
                yield create_sse_data({
                    'type': 'start',
                    'session_id': session_id,
                    'project_id': project_id,
                    'project_name': project.name,
                    'requirement': user_message_content,
                    'context_limit': compression_settings.max_context_tokens if compression_settings else 128000
                })
                
                # 8. æµå¼æ‰§è¡ŒStateGraph
                step_count = 0
                invoke_config = {
                    "configurable": {"thread_id": thread_id},
                    "recursion_limit": 1000  # æ”¯æŒçº¦500æ¬¡å·¥å…·è°ƒç”¨
                }
                
                # ğŸ” DEBUG: æ£€æŸ¥checkpointerä¸­çš„å†å²çŠ¶æ€
                try:
                    checkpoint_tuple = await checkpointer.aget(invoke_config)
                    if checkpoint_tuple:
                        # checkpoint_tupleæ˜¯ä¸€ä¸ªtuple: (checkpoint_dict, metadata)
                        checkpoint_dict = checkpoint_tuple[0] if isinstance(checkpoint_tuple, tuple) else checkpoint_tuple
                        if checkpoint_dict and isinstance(checkpoint_dict, dict):
                            channel_values = checkpoint_dict.get("channel_values", {})
                            history_messages = channel_values.get("messages", [])
                            logger.info(f"ğŸ” DEBUG: Found {len(history_messages)} history messages in checkpointer for thread_id={thread_id}")
                        else:
                            logger.info(f"ğŸ” DEBUG: Checkpoint structure: {type(checkpoint_tuple)}")
                    else:
                        logger.info(f"ğŸ” DEBUG: No checkpoint found for thread_id={thread_id}, starting new conversation")
                except Exception as e:
                    logger.warning(f"ğŸ” DEBUG: Failed to check history: {e}")
                
                # ä½¿ç”¨astream_eventsè¿›è¡Œæµå¼å¤„ç†,æ•è·å·¥å…·è°ƒç”¨ç­‰æ‰€æœ‰äº‹ä»¶(v2ç‰ˆæœ¬æ”¯æŒåµŒå¥—Agent)
                final_state = None
                # 6. å‡†å¤‡æµå¼äº‹ä»¶æ•è·
                current_node_name = None  # è·Ÿè¸ªå½“å‰æ‰§è¡Œçš„èŠ‚ç‚¹
                
                try:
                    # checkpointerä¼šè‡ªåŠ¨ï¼š
                    # 1. åŠ è½½thread_idå¯¹åº”çš„å†å²çŠ¶æ€
                    # 2. å°†input_stateä¸­çš„messagesè¿½åŠ åˆ°å†å²messages
                    # 3. æ›´æ–°å…¶ä»–å­—æ®µï¼ˆè¦†ç›–æ–¹å¼ï¼‰
                    async for event in graph.astream_events(
                        input_state,
                        config=invoke_config,
                        version="v2"  # v2ç‰ˆæœ¬æ”¯æŒæ•è·åµŒå¥—Runnableçš„äº‹ä»¶
                    ):
                        event_type = event.get("event")
                        event_name = event.get("name", "")
                        event_data = event.get("data", {})
                        
                        # ğŸ” DEBUG: æ‰“å°æ‰€æœ‰äº‹ä»¶ç±»å‹ä»¥ä¾¿è¯Šæ–­ï¼ˆå¯é€‰ï¼‰
                        # if "tool" in event_name.lower() or "tool" in event_type:
                        #     logger.info(f"ğŸ” Event: {event_type}, Name: {event_name}, Data keys: {event_data.keys() if isinstance(event_data, dict) else type(event_data)}")
                        
                        # 1. LLMä»¤ç‰Œæµå¼ä¼ è¾“ (on_chat_model_stream) - é€å­—/é€è¯æµå¼è¾“å‡º
                        if event_type == "on_chat_model_stream":
                            chunk = event_data.get("chunk", {})
                            metadata = event_data.get("metadata", {})
                            content = ""
                            if hasattr(chunk, 'content'):
                                content = chunk.content  # æå–contentå±æ€§
                            elif isinstance(chunk, dict) and 'content' in chunk:
                                content = chunk['content']
                            
                            # ğŸ”§ è¿‡æ»¤Brainçš„åŸå§‹JSONè¾“å‡ºï¼ˆåªæ˜¾ç¤ºæ ¼å¼åŒ–å†³ç­–ï¼‰
                            # current_node_nameåœ¨on_chain_startæ—¶è®¾ç½®ä¸ºé¡¶å±‚èŠ‚ç‚¹åç§°ï¼ˆå¦‚"brain_agent"ï¼‰
                            if content and not (current_node_name and "brain" in current_node_name.lower()):
                                # ä¸æ˜¯BrainèŠ‚ç‚¹ï¼Œæ­£å¸¸å‘é€ï¼ˆchat/requirement/testcaseç­‰agentçš„è¾“å‡ºï¼‰
                                # æå–agentåç§°ç”¨äºæ˜¾ç¤º
                                agent_name = current_node_name.replace('_agent', '').title() if current_node_name else 'Unknown'
                                
                                yield create_sse_data({
                                    'type': 'message', 
                                    'data': {
                                        'content': content,
                                        'additional_kwargs': {
                                            'agent': agent_name.lower(),
                                            'agent_type': 'orchestrator_agent',
                                            'node_name': current_node_name
                                        },
                                        'response_metadata': metadata,
                                        'id': metadata.get('run_id', 'unknown')
                                    }
                                })
                                
                                # ğŸ”§ æ—¥å¿—ï¼šæ˜¾ç¤ºæµå¼è¾“å‡ºçš„è¯¦ç»†ä¿¡æ¯
                                if len(content) > 0:
                                    logger.debug(f"[{agent_name} Stream] Token: '{content}' (length={len(content)})")
                            # BrainèŠ‚ç‚¹çš„on_chat_model_streamä»¤ç‰Œè¢«è·³è¿‡
                            # æ ¼å¼åŒ–å†³ç­–æ¶ˆæ¯åœ¨on_chain_endäº‹ä»¶ä¸­å‘é€
                        
                        # 2. å·¥å…·è°ƒç”¨å¼€å§‹ (on_tool_start)
                        elif event_type == "on_tool_start":
                            tool_name = event_name
                            tool_input = event_data.get("input", {})
                            logger.info(f"OrchestratorStream: Tool {tool_name} started with input: {tool_input}")
                            
                            # ä¼˜åŒ–å·¥å…·å‚æ•°æ˜¾ç¤º
                            if not tool_input or tool_input == {}:
                                tool_input_display = "æ— éœ€å‚æ•°"
                                tool_input_detail = "è¯¥å·¥å…·ä¸éœ€è¦è¾“å…¥å‚æ•°"
                            else:
                                tool_input_display = tool_input
                                # æ ¼å¼åŒ–å‚æ•°è¯¦æƒ…
                                param_details = [f"{k}: {v}" for k, v in tool_input.items()]
                                tool_input_detail = ", ".join(param_details)
                            
                            yield create_sse_data({
                                'type': 'tool_start',
                                'tool_name': tool_name,
                                'tool_input': tool_input_display,
                                'tool_input_detail': tool_input_detail
                            })
                        
                        # 3. å·¥å…·è°ƒç”¨ç»“æŸ (on_tool_end)
                        elif event_type == "on_tool_end":
                            tool_name = event_name
                            tool_output = event_data.get("output")
                            
                            # æå–å·¥å…·è¾“å‡ºçš„å®é™…å†…å®¹
                            if hasattr(tool_output, 'content'):
                                actual_content = tool_output.content
                            else:
                                actual_content = str(tool_output)
                            
                            logger.info(f"OrchestratorStream: Tool {tool_name} completed with output: {str(actual_content)[:200]}")
                            
                            # å®Œæ•´å‘é€å·¥å…·è¾“å‡º,å‰ç«¯å¯ä»¥è‡ªè¡Œæˆªæ–­æ˜¾ç¤º
                            output_length = len(actual_content)
                            
                            yield create_sse_data({
                                'type': 'tool_end',
                                'tool_name': tool_name,
                                'tool_output': actual_content,  # å®Œæ•´è¾“å‡º
                                'output_length': output_length
                            })
                        
                        # 3.5. å·¥å…·è°ƒç”¨å‡ºé”™ (on_tool_error)
                        elif event_type == "on_tool_error":
                            tool_name = event_name
                            error_info = event_data.get("error") or str(event_data)
                            logger.error(f"OrchestratorStream: Tool {tool_name} failed with error: {error_info}")
                            yield create_sse_data({
                                'type': 'tool_error',
                                'tool_name': tool_name,
                                'error': str(error_info)
                            })
                        
                        # 4. èŠ‚ç‚¹å¼€å§‹æ‰§è¡Œ (on_chain_start)
                        # ğŸ”§ ä¿®å¤ï¼šåªåœ¨é¡¶å±‚èŠ‚ç‚¹ï¼ˆ_agentç»“å°¾ï¼‰æ—¶è®¾ç½®current_node_nameï¼Œå¿½ç•¥å†…éƒ¨Runnable
                        elif event_type == "on_chain_start" and event_name.endswith("_agent"):
                            current_node_name = event_name
                            logger.info(f"OrchestratorStream: Node {current_node_name} started")
                            
                            # ğŸ”§ æ–°å¢ï¼šèŠ‚ç‚¹å¼€å§‹æ—¶å‘é€åˆ†éš”æ ‡è®°ï¼Œè®©å‰ç«¯çŸ¥é“æ–°çš„agentå¼€å§‹äº†
                            if "brain" not in event_name.lower():
                                yield create_sse_data({
                                    'type': 'agent_start',
                                    'agent': event_name.replace('_agent', '').title()
                                })
                        
                        # 5. èŠ‚ç‚¹æ‰§è¡Œç»“æŸ (on_chain_end) - å¤„ç†å„Agentçš„è¾“å‡º
                        elif event_type == "on_chain_end" and "agent" in event_name:
                            node_name = event_name
                            node_output = event_data.get("output", {})
                            logger.info(f"OrchestratorStream: Node {node_name} completed")
                            
                            # ğŸ”§ ä¿®å¤ï¼šèŠ‚ç‚¹ç»“æŸæ—¶æ¸…ç©ºcurrent_node_nameï¼Œé¿å…å½±å“ä¸‹ä¸€ä¸ªèŠ‚ç‚¹çš„æµå¼è¾“å‡º
                            if current_node_name == node_name:
                                current_node_name = None
                            
                            if "brain" in node_name and isinstance(node_output, dict):
                                # BrainèŠ‚ç‚¹å®Œæˆ
                                next_agent = node_output.get("next_agent", "")
                                instruction = node_output.get("instruction", "")
                                reason = node_output.get("reason", "")
                                current_step = node_output.get("current_step", 0)
                                
                                # è·å–Tokenä½¿ç”¨ä¿¡æ¯
                                context_token_count = node_output.get("context_token_count", 0)
                                context_limit = compression_settings.max_context_tokens if compression_settings else 128000
                                logger.info(f"[Context Update] Sending token info: {context_token_count}/{context_limit}")
                                
                                # è·å–çŠ¶æ€ä¿¡æ¯ä»¥åŒ¹é…å†å²æ ¼å¼
                                executed_agents = node_output.get("executed_agents", [])
                                has_requirement_analysis = bool(node_output.get("requirement_analysis"))
                                has_testcases = bool(node_output.get("testcases"))
                                max_steps = node_output.get("max_steps", 10)
                                
                                # å‘é€ä¸Šä¸‹æ–‡Tokenæ›´æ–°äº‹ä»¶
                                yield create_sse_data({
                                    'type': 'context_update',
                                    'context_token_count': context_token_count,
                                    'context_limit': context_limit
                                })
                                
                                # ğŸ”§ å…³é”®ï¼šBrainå¯èƒ½è¿”å›å¤šæ¡æ¶ˆæ¯ï¼ˆdecision + final_responseï¼‰
                                messages_from_brain = node_output.get("messages", [])
                                
                                # éå†æ‰€æœ‰æ¶ˆæ¯å¹¶å‘é€
                                for msg in messages_from_brain:
                                    from langchain_core.messages import AIMessage
                                    if isinstance(msg, AIMessage):
                                        agent_type = msg.additional_kwargs.get("agent_type", "")
                                        
                                        if agent_type == "orchestrator_brain_decision":
                                            # Brainå†³ç­–æ¶ˆæ¯
                                            # å…ˆå‘é€æ ¼å¼åŒ–æ–‡æœ¬æ¶ˆæ¯ï¼ˆä¸å†å²å­˜å‚¨ä¸€è‡´ï¼‰
                                            yield create_sse_data({
                                                'type': 'message',
                                                'data': {
                                                    'content': msg.content,
                                                    'additional_kwargs': msg.additional_kwargs,
                                                    'response_metadata': {},
                                                    'id': 'brain_decision'
                                                }
                                            })
                                            
                                            # å†å‘é€ç»“æ„åŒ–å†³ç­–æ•°æ®ï¼ˆä¾›å‰ç«¯ç‰¹æ®Šå¤„ç†ï¼‰
                                            yield create_sse_data({
                                                'type': 'brain_decision',
                                                'agent': 'Brain',
                                                'next_agent': next_agent,
                                                'instruction': instruction,
                                                'reason': reason,
                                                'step': current_step
                                            })
                                        
                                        elif agent_type == "orchestrator_final_response":
                                            # æœ€ç»ˆç”¨æˆ·å›å¤æ¶ˆæ¯
                                            logger.info(f"OrchestratorStream: Sending final response to user")
                                            yield create_sse_data({
                                                'type': 'message',
                                                'data': {
                                                    'content': msg.content,
                                                    'additional_kwargs': msg.additional_kwargs,
                                                    'response_metadata': {},
                                                    'id': 'final_response'
                                                }
                                            })
                                
                                if next_agent == "END":
                                    logger.info(f"OrchestratorStream: Brain decided to END")
                            
                            elif "chat" in node_name and isinstance(node_output, dict):
                                # Chat Agentå®Œæˆ
                                yield create_sse_data({
                                    'type': 'chat_response',
                                    'agent': 'Chat'
                                })
                            
                            elif "requirement" in node_name and isinstance(node_output, dict):
                                # Requirement Agentå®Œæˆ
                                analysis = node_output.get("requirement_analysis", {})
                                yield create_sse_data({
                                    'type': 'requirement_analysis',
                                    'agent': 'Requirement',
                                    'analysis': analysis
                                })
                            
                            elif "testcase" in node_name and isinstance(node_output, dict):
                                # TestCase Agentå®Œæˆ
                                testcases = node_output.get("testcases", [])
                                yield create_sse_data({
                                    'type': 'testcase_generation',
                                    'agent': 'TestCase',
                                    'testcase_count': len(testcases),
                                    'testcases': testcases
                                })
                            
                            # ä¿å­˜æœ€ç»ˆçŠ¶æ€
                            if final_state is None:
                                final_state = {}
                            # ğŸ”§ ä¿®å¤ï¼šæ£€æŸ¥node_outputæ˜¯å¦ä¸ºNoneæˆ–dict
                            if node_output and isinstance(node_output, dict):
                                final_state.update(node_output)
                        
                        # æ·»åŠ å°å»¶è¿Ÿä»¥ç¡®ä¿æµå¼ä¼ è¾“æ•ˆæœ
                        await asyncio.sleep(0.01)
                
                except Exception as e:
                    logger.error(f"OrchestratorStream: Error during streaming: {e}", exc_info=True)
                    yield create_sse_data({'type': 'error', 'message': f'Streaming error: {str(e)}'})
                
                # 9. æ„å»ºæœ€ç»ˆç»“æœæ‘˜è¦
                final_values = final_state or {}
                
                # 10. å‘é€æœ€ç»ˆæ‘˜è¦
                if final_values:
                    yield create_sse_data({
                        'type': 'final_summary',
                        'requirement_analysis': final_values.get('requirement_analysis'),
                        'knowledge_doc_count': len(final_values.get('knowledge_docs', [])),
                        'testcase_count': len(final_values.get('testcases', [])),
                        'total_steps': final_values.get('current_step', 0)
                    })
                
                # 11. ä¿å­˜ä»»åŠ¡è®°å½•åˆ°æ•°æ®åº“
                try:
                    task_data = {
                        'user': request.user,
                        'project_id': project_id,
                        'requirement': user_message_content,
                        'status': 'completed',
                        'requirement_analysis': final_values.get('requirement_analysis'),
                        'knowledge_docs': final_values.get('knowledge_docs', []),
                        'testcases': final_values.get('testcases', []),
                    }
                    
                    try:
                        chat_session = await sync_to_async(ChatSession.objects.get)(
                            session_id=session_id,
                            user=request.user
                        )
                        task_data['chat_session'] = chat_session
                    except ChatSession.DoesNotExist:
                        pass
                    
                    await sync_to_async(OrchestratorTask.objects.create)(**task_data)
                    logger.info(f"OrchestratorStream: Task record saved")
                except Exception as e:
                    logger.error(f"OrchestratorStream: Failed to save task record: {e}")
                
                # 12. å‘é€å®Œæˆä¿¡å·
                yield create_sse_data({'type': 'complete'})
                yield "data: [DONE]\n\n"
            
        except Exception as e:
            logger.error(f"OrchestratorStream: Error in stream generator: {e}", exc_info=True)
            yield create_sse_data({'type': 'error', 'message': f'Stream error: {str(e)}'})
    
    async def post(self, request, *args, **kwargs):
        """å¤„ç†æµå¼ç¼–æ’è¯·æ±‚"""
        try:
            user = await self.authenticate_request(request)
            request.user = user
            logger.info(f"OrchestratorStream: Request from user {user.id}")
        except AuthenticationFailed as e:
            error_data = create_sse_data({
                'type': 'error',
                'message': str(e),
                'code': 401
            })
            return StreamingHttpResponse(
                iter([error_data]),
                content_type='text/event-stream; charset=utf-8',
                status=401
            )
        
        try:
            body_data = json.loads(request.body.decode('utf-8'))
        except (json.JSONDecodeError, UnicodeDecodeError) as e:
            error_data = create_sse_data({
                'type': 'error',
                'message': f'Invalid JSON data: {str(e)}',
                'code': 400
            })
            return StreamingHttpResponse(
                iter([error_data]),
                content_type='text/event-stream; charset=utf-8',
                status=400
            )
        
        user_message_content = body_data.get('message')
        session_id = body_data.get('session_id')
        project_id = body_data.get('project_id')
        prompt_id = body_data.get('prompt_id')
        
        if not project_id:
            error_data = create_sse_data({
                'type': 'error',
                'message': 'project_id is required',
                'code': 400
            })
            return StreamingHttpResponse(
                iter([error_data]),
                content_type='text/event-stream; charset=utf-8',
                status=400
            )
        
        if not user_message_content:
            error_data = create_sse_data({
                'type': 'error',
                'message': 'message is required',
                'code': 400
            })
            return StreamingHttpResponse(
                iter([error_data]),
                content_type='text/event-stream; charset=utf-8',
                status=400
            )
        
        project = await sync_to_async(self._check_project_permission)(request.user, project_id)
        if not project:
            error_data = create_sse_data({
                'type': 'error',
                'message': "You don't have permission to access this project",
                'code': 403
            })
            return StreamingHttpResponse(
                iter([error_data]),
                content_type='text/event-stream; charset=utf-8',
                status=403
            )
        
        if not session_id:
            session_id = uuid.uuid4().hex
            logger.info(f"OrchestratorStream: Generated session_id: {session_id}")
        
        async def async_generator():
            async for chunk in self._create_sse_generator(
                request, user_message_content, session_id,
                project_id, project, prompt_id
            ):
                yield chunk
        
        response = StreamingHttpResponse(
            async_generator(),
            content_type='text/event-stream; charset=utf-8'
        )
        response['Cache-Control'] = 'no-cache'
        response['X-Accel-Buffering'] = 'no'
        response['Access-Control-Allow-Origin'] = '*'
        
        return response
