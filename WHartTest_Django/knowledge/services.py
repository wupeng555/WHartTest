"""
çŸ¥è¯†åº“æœåŠ¡æ¨¡å—
æä¾›æ–‡æ¡£å¤„ç†ã€å‘é‡åŒ–ã€æ£€ç´¢ç­‰æ ¸å¿ƒåŠŸèƒ½
"""
import os
import time
import hashlib
from typing import List, Dict, Any
import nltk
from django.conf import settings

# --- NLTK æ•°æ®è·¯å¾„é…ç½® ---
# å°†é¡¹ç›®å†…éƒ¨çš„ 'nltk_data' ç›®å½•æ·»åŠ åˆ° NLTK çš„æœç´¢è·¯å¾„ä¸­
# è¿™ä½¿å¾—é¡¹ç›®åœ¨ä»»ä½•ç¯å¢ƒä¸­éƒ½èƒ½æ‰¾åˆ°å¿…è¦çš„æ•°æ®ï¼Œæ— éœ€ç³»ç»Ÿçº§å®‰è£…
LOCAL_NLTK_DATA_PATH = os.path.join(settings.BASE_DIR, 'nltk_data')
if os.path.exists(LOCAL_NLTK_DATA_PATH):
    if LOCAL_NLTK_DATA_PATH not in nltk.data.path:
        nltk.data.path.insert(0, LOCAL_NLTK_DATA_PATH)
        print(f"NLTK data path prepended with: {LOCAL_NLTK_DATA_PATH}")

# è®¾ç½®å®Œå…¨ç¦»çº¿æ¨¡å¼ï¼Œé¿å…ä»»ä½•ç½‘ç»œè¯·æ±‚
os.environ['TRANSFORMERS_OFFLINE'] = '1'
os.environ['HF_DATASETS_OFFLINE'] = '1'
os.environ['HF_HUB_OFFLINE'] = '1'
os.environ['TOKENIZERS_PARALLELISM'] = 'false'
# ç¦ç”¨ç½‘ç»œè¿æ¥
os.environ['HF_HUB_DISABLE_TELEMETRY'] = '1'
os.environ['HF_HUB_DISABLE_PROGRESS_BARS'] = '1'
os.environ['HF_HUB_DISABLE_SYMLINKS_WARNING'] = '1'
# è®¾ç½®æçŸ­çš„è¿æ¥è¶…æ—¶ï¼Œå¼ºåˆ¶å¿«é€Ÿå¤±è´¥
os.environ['HF_HUB_TIMEOUT'] = '1'
os.environ['REQUESTS_TIMEOUT'] = '1'
from django.conf import settings
from django.utils import timezone
from langchain_community.document_loaders import (
    PyPDFLoader, Docx2txtLoader, UnstructuredPowerPointLoader,
    TextLoader, UnstructuredMarkdownLoader, UnstructuredHTMLLoader,
    WebBaseLoader
)
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_chroma import Chroma
from langchain_core.documents import Document as LangChainDocument
from .models import KnowledgeBase, Document, DocumentChunk, QueryLog
import logging
import requests
from typing import List
from langchain.embeddings.base import Embeddings

logger = logging.getLogger(__name__)


class CustomAPIEmbeddings(Embeddings):
    """è‡ªå®šä¹‰HTTP APIåµŒå…¥æœåŠ¡"""
    
    def __init__(self, api_base_url: str, api_key: str = None, custom_headers: dict = None, model_name: str = 'text-embedding'):
        self.api_base_url = api_base_url.rstrip('/')
        self.api_key = api_key
        self.custom_headers = custom_headers or {}
        self.model_name = model_name
        
    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        """åµŒå…¥å¤šä¸ªæ–‡æ¡£"""
        return [self.embed_query(text) for text in texts]
    
    def embed_query(self, text: str) -> List[float]:
        """åµŒå…¥å•ä¸ªæŸ¥è¯¢"""
        headers = {
            'Content-Type': 'application/json',
            **self.custom_headers
        }
        
        if self.api_key:
            headers['Authorization'] = f'Bearer {self.api_key}'
        
        data = {
            'input': text,
            'model': self.model_name  # ä½¿ç”¨é…ç½®çš„æ¨¡å‹å
        }
        
        try:
            response = requests.post(
                self.api_base_url,  # ç›´æ¥ä½¿ç”¨å®Œæ•´çš„API URL
                json=data,
                headers=headers,
                timeout=30
            )
            response.raise_for_status()
            
            result = response.json()
            if 'data' in result and len(result['data']) > 0:
                return result['data'][0]['embedding']
            else:
                raise ValueError(f"APIå“åº”æ ¼å¼é”™è¯¯: {result}")
                
        except Exception as e:
            raise RuntimeError(f"è‡ªå®šä¹‰APIåµŒå…¥å¤±è´¥: {str(e)}")




class DocumentProcessor:
    """æ–‡æ¡£å¤„ç†å™¨"""

    def __init__(self):
        self.loaders = {
            'pdf': PyPDFLoader,
            'docx': Docx2txtLoader,
            'pptx': UnstructuredPowerPointLoader,
            'txt': TextLoader,
            'md': UnstructuredMarkdownLoader,
            'html': UnstructuredHTMLLoader,
        }

    def load_document(self, document: Document) -> List[LangChainDocument]:
        """åŠ è½½æ–‡æ¡£å†…å®¹"""
        try:
            logger.info(f"å¼€å§‹åŠ è½½æ–‡æ¡£: {document.title} (ID: {document.id})")
            logger.info(f"æ–‡æ¡£ç±»å‹: {document.document_type}")

            # ä¼˜å…ˆçº§ï¼šURL > æ–‡æœ¬å†…å®¹ > æ–‡ä»¶
            if document.document_type == 'url' and document.url:
                logger.info(f"ä»URLåŠ è½½: {document.url}")
                return self._load_from_url(document.url)
            elif document.content:
                # å¦‚æœæœ‰æ–‡æœ¬å†…å®¹ï¼Œç›´æ¥ä½¿ç”¨
                logger.info("ä»æ–‡æœ¬å†…å®¹åŠ è½½")
                return self._load_from_content(document.content, document.title)
            elif document.file and hasattr(document.file, 'path'):
                file_path = document.file.path
                logger.info(f"ä»æ–‡ä»¶åŠ è½½: {file_path}")

                # Windowsè·¯å¾„å…¼å®¹æ€§å¤„ç†
                if os.name == 'nt':  # Windowsç³»ç»Ÿ
                    file_path = os.path.normpath(file_path)
                    if not os.path.isabs(file_path):
                        file_path = os.path.abspath(file_path)

                # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
                if os.path.exists(file_path):
                    logger.info(f"æ–‡ä»¶å­˜åœ¨ï¼Œå¼€å§‹åŠ è½½: {file_path}")
                    return self._load_from_file(document)
                else:
                    raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            else:
                raise ValueError("æ–‡æ¡£æ²¡æœ‰å¯ç”¨çš„å†…å®¹æºï¼ˆæ— URLã€æ— æ–‡æœ¬å†…å®¹ã€æ— æ–‡ä»¶ï¼‰")
        except Exception as e:
            logger.error(f"åŠ è½½æ–‡æ¡£å¤±è´¥ {document.id}: {e}")
            raise

    def _load_from_url(self, url: str) -> List[LangChainDocument]:
        """ä»URLåŠ è½½æ–‡æ¡£"""
        loader = WebBaseLoader(url)
        return loader.load()

    def _load_from_content(self, content: str, title: str) -> List[LangChainDocument]:
        """ä»æ–‡æœ¬å†…å®¹åŠ è½½æ–‡æ¡£"""
        return [LangChainDocument(
            page_content=content,
            metadata={"source": title, "title": title}
        )]

    def _load_from_file(self, document: Document) -> List[LangChainDocument]:
        """ä»æ–‡ä»¶åŠ è½½æ–‡æ¡£"""
        file_path = document.file.path

        # Windowsè·¯å¾„å…¼å®¹æ€§å¤„ç†
        if os.name == 'nt':  # Windowsç³»ç»Ÿ
            # ç¡®ä¿è·¯å¾„ä½¿ç”¨æ­£ç¡®çš„åˆ†éš”ç¬¦
            file_path = os.path.normpath(file_path)
            # è½¬æ¢ä¸ºç»å¯¹è·¯å¾„
            if not os.path.isabs(file_path):
                file_path = os.path.abspath(file_path)

        logger.info(f"å°è¯•åŠ è½½æ–‡ä»¶: {file_path}")

        # å†æ¬¡æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")

        loader_class = self.loaders.get(document.document_type)
        if not loader_class:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–‡æ¡£ç±»å‹: {document.document_type}")

        try:
            # å¯¹äºæ–‡æœ¬æ–‡ä»¶ï¼Œä½¿ç”¨UTF-8ç¼–ç 
            if document.document_type == 'txt':
                loader = loader_class(file_path, encoding='utf-8')
            else:
                loader = loader_class(file_path)

            docs = loader.load()

            # æ£€æŸ¥æ˜¯å¦æˆåŠŸåŠ è½½å†…å®¹
            if not docs:
                raise ValueError(f"æ–‡æ¡£åŠ è½½å¤±è´¥ï¼Œæ²¡æœ‰å†…å®¹: {file_path}")

            logger.info(f"æˆåŠŸåŠ è½½æ–‡æ¡£ï¼Œé¡µæ•°: {len(docs)}")

            # æ·»åŠ å…ƒæ•°æ®
            for doc in docs:
                doc.metadata.update({
                    "source": document.title,
                    "document_id": str(document.id),
                    "document_type": document.document_type,
                    "title": document.title,
                    "file_path": file_path
                })

            return docs

        except Exception as e:
            logger.error(f"æ–‡æ¡£åŠ è½½å™¨å¤±è´¥: {e}")
            # å¦‚æœæ˜¯æ–‡æœ¬æ–‡ä»¶ï¼Œå°è¯•ç›´æ¥è¯»å–
            if document.document_type == 'txt':
                try:
                    logger.info("å°è¯•ç›´æ¥è¯»å–æ–‡æœ¬æ–‡ä»¶...")
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read()

                    if not content.strip():
                        raise ValueError("æ–‡ä»¶å†…å®¹ä¸ºç©º")

                    return [LangChainDocument(
                        page_content=content,
                        metadata={
                            "source": document.title,
                            "document_id": str(document.id),
                            "document_type": document.document_type,
                            "title": document.title,
                            "file_path": file_path
                        }
                    )]
                except Exception as read_error:
                    logger.error(f"ç›´æ¥è¯»å–æ–‡ä»¶ä¹Ÿå¤±è´¥: {read_error}")
                    raise
            else:
                raise


class VectorStoreManager:
    """å‘é‡å­˜å‚¨ç®¡ç†å™¨"""

    # ç±»çº§åˆ«çš„å‘é‡å­˜å‚¨ç¼“å­˜
    _vector_store_cache = {}
    _embeddings_cache = {}

    def __init__(self, knowledge_base: KnowledgeBase):
        self.knowledge_base = knowledge_base
        self.embeddings = self._get_embeddings_instance(knowledge_base)
        self._log_embedding_info()

    def _get_embeddings_instance(self, knowledge_base):
        """è·å–åµŒå…¥æ¨¡å‹å®ä¾‹ï¼Œæ”¯æŒå¤šç§æœåŠ¡ç±»å‹"""
        cache_key = f"{knowledge_base.embedding_service}_{knowledge_base.id}"
        if cache_key not in self._embeddings_cache:
            embedding_service = knowledge_base.embedding_service
            
            try:
                if embedding_service == 'openai':
                    # OpenAI Embeddings
                    self._embeddings_cache[cache_key] = self._create_openai_embeddings(knowledge_base)
                elif embedding_service == 'azure_openai':
                    # Azure OpenAI Embeddings
                    self._embeddings_cache[cache_key] = self._create_azure_embeddings(knowledge_base)
                elif embedding_service == 'ollama':
                    # Ollama Embeddings
                    self._embeddings_cache[cache_key] = self._create_ollama_embeddings(knowledge_base)
                elif embedding_service == 'custom':
                    # è‡ªå®šä¹‰HTTP API
                    self._embeddings_cache[cache_key] = self._create_custom_api_embeddings(knowledge_base)
                else:
                    # ä¸æ”¯æŒçš„åµŒå…¥æœåŠ¡
                    raise ValueError(f"ä¸æ”¯æŒçš„åµŒå…¥æœåŠ¡: {embedding_service}")
                    
                # æµ‹è¯•åµŒå…¥åŠŸèƒ½
                test_embedding = self._embeddings_cache[cache_key].embed_query("æ¨¡å‹åŠŸèƒ½æµ‹è¯•")
                logger.info(f"âœ… åµŒå…¥æ¨¡å‹æµ‹è¯•æˆåŠŸ: {embedding_service}, ç»´åº¦: {len(test_embedding)}")
                
            except Exception as e:
                logger.error(f"âŒ åµŒå…¥æœåŠ¡ {embedding_service} åˆå§‹åŒ–å¤±è´¥: {str(e)}")
                raise
                
        return self._embeddings_cache[cache_key]
    
    def _create_openai_embeddings(self, knowledge_base):
        """åˆ›å»ºOpenAI Embeddingså®ä¾‹"""
        try:
            from langchain_openai import OpenAIEmbeddings
        except ImportError:
            raise ImportError("éœ€è¦å®‰è£…langchain-openai: pip install langchain-openai")
        
        kwargs = {
            'model': knowledge_base.model_name or 'text-embedding-ada-002',
        }
        
        if knowledge_base.api_key:
            kwargs['api_key'] = knowledge_base.api_key
        if knowledge_base.api_base_url:
            kwargs['base_url'] = knowledge_base.api_base_url
            
        logger.info(f"ğŸš€ åˆå§‹åŒ–OpenAIåµŒå…¥æ¨¡å‹: {kwargs['model']}")
        return OpenAIEmbeddings(**kwargs)
    
    def _create_azure_embeddings(self, knowledge_base):
        """åˆ›å»ºAzure OpenAI Embeddingså®ä¾‹"""
        try:
            from langchain_openai import AzureOpenAIEmbeddings
        except ImportError:
            raise ImportError("éœ€è¦å®‰è£…langchain-openai: pip install langchain-openai")
        
        if not all([knowledge_base.api_key, knowledge_base.api_base_url]):
            raise ValueError("Azure OpenAIéœ€è¦é…ç½®api_keyå’Œapi_base_url")
        
        kwargs = {
            'model': knowledge_base.model_name or 'text-embedding-ada-002',
            'api_key': knowledge_base.api_key,
            'azure_endpoint': knowledge_base.api_base_url,
            'api_version': '2024-02-15-preview',  # é»˜è®¤ç‰ˆæœ¬
        }
        
        # éƒ¨ç½²åé»˜è®¤ä½¿ç”¨æ¨¡å‹å
        kwargs['deployment'] = knowledge_base.model_name or 'text-embedding-ada-002'
            
        logger.info(f"ğŸš€ åˆå§‹åŒ–Azure OpenAIåµŒå…¥æ¨¡å‹: {kwargs['model']}")
        return AzureOpenAIEmbeddings(**kwargs)
    
    def _create_ollama_embeddings(self, knowledge_base):
        """åˆ›å»ºOllama Embeddingså®ä¾‹"""
        try:
            from langchain_ollama import OllamaEmbeddings
        except ImportError:
            raise ImportError("éœ€è¦å®‰è£…langchain-ollama: pip install langchain-ollama")
        
        kwargs = {
            'model': knowledge_base.model_name or 'nomic-embed-text',
        }
        
        if knowledge_base.api_base_url:
            kwargs['base_url'] = knowledge_base.api_base_url
        else:
            kwargs['base_url'] = 'http://localhost:11434'  # Ollamaé»˜è®¤åœ°å€
            
        logger.info(f"ğŸš€ åˆå§‹åŒ–OllamaåµŒå…¥æ¨¡å‹: {kwargs['model']}")
        return OllamaEmbeddings(**kwargs)
    
    def _create_custom_api_embeddings(self, knowledge_base):
        """åˆ›å»ºè‡ªå®šä¹‰API Embeddingså®ä¾‹"""
        if not knowledge_base.api_base_url:
            raise ValueError("è‡ªå®šä¹‰APIéœ€è¦é…ç½®api_base_url")
        
        logger.info(f"ğŸš€ åˆå§‹åŒ–è‡ªå®šä¹‰APIåµŒå…¥æ¨¡å‹: {knowledge_base.api_base_url}")
        return CustomAPIEmbeddings(
            api_base_url=knowledge_base.api_base_url,
            api_key=knowledge_base.api_key,
            custom_headers={},  # ä¸å†ä½¿ç”¨æ•°æ®åº“ä¸­çš„custom_headerså­—æ®µ
            model_name=knowledge_base.model_name
        )
    
    def _log_embedding_info(self):
        """è®°å½•åµŒå…¥æ¨¡å‹ä¿¡æ¯"""
        embedding_type = type(self.embeddings).__name__
        logger.info(f"   ğŸŒŸ çŸ¥è¯†åº“: {self.knowledge_base.name}")
        logger.info(f"   ğŸ¯ é…ç½®çš„åµŒå…¥æ¨¡å‹: {self.knowledge_base.model_name}")
        logger.info(f"   âœ… å®é™…ä½¿ç”¨çš„åµŒå…¥æ¨¡å‹: {embedding_type}")

        # æ¨¡å‹è¯´æ˜
        if embedding_type == "OpenAIEmbeddings":
            logger.info(f"   ğŸ‰ è¯´æ˜: ä½¿ç”¨OpenAIåµŒå…¥APIæœåŠ¡")
        elif embedding_type == "AzureOpenAIEmbeddings":
            logger.info(f"   ğŸ‰ è¯´æ˜: ä½¿ç”¨Azure OpenAIåµŒå…¥APIæœåŠ¡")
        elif embedding_type == "OllamaEmbeddings":
            logger.info(f"   ğŸ‰ è¯´æ˜: ä½¿ç”¨Ollamaæœ¬åœ°APIåµŒå…¥æœåŠ¡")
        elif embedding_type == "CustomAPIEmbeddings":
            logger.info(f"   ğŸ‰ è¯´æ˜: ä½¿ç”¨è‡ªå®šä¹‰HTTP APIåµŒå…¥æœåŠ¡")

        self._vector_store = None
        embedding_type = type(self.embeddings).__name__
        logger.info(f"ğŸ¤– å‘é‡å­˜å‚¨ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ:")
        logger.info(f"   ğŸ“‹ çŸ¥è¯†åº“: {self.knowledge_base.name} (ID: {self.knowledge_base.id})")
        logger.info(f"   ğŸ¯ é…ç½®çš„åµŒå…¥æ¨¡å‹: {self.knowledge_base.model_name}")
        logger.info(f"   âœ… å®é™…ä½¿ç”¨çš„åµŒå…¥æ¨¡å‹: {embedding_type}")
        logger.info(f"   ğŸ’¾ å‘é‡å­˜å‚¨ç±»å‹: ChromaDB")

    @property
    def vector_store(self):
        """è·å–å‘é‡å­˜å‚¨å®ä¾‹ï¼ˆå¸¦ç¼“å­˜å’Œå¥åº·æ£€æŸ¥ï¼‰"""
        if self._vector_store is None:
            # ä½¿ç”¨çŸ¥è¯†åº“IDä½œä¸ºç¼“å­˜é”®
            cache_key = str(self.knowledge_base.id)

            if cache_key in self._vector_store_cache:
                # éªŒè¯ç¼“å­˜çš„å®ä¾‹æ˜¯å¦ä»ç„¶æœ‰æ•ˆ
                cached_store = self._vector_store_cache[cache_key]
                try:
                    # å°è¯•è®¿é—® Collection,éªŒè¯å…¶å­˜åœ¨æ€§
                    _ = cached_store._collection.count()
                    logger.info(f"ä½¿ç”¨ç¼“å­˜çš„å‘é‡å­˜å‚¨å®ä¾‹: {cache_key}")
                    self._vector_store = cached_store
                except Exception as e:
                    logger.warning(f"ç¼“å­˜çš„ Collection æ— æ•ˆ,é‡æ–°åˆ›å»º: {e}")
                    # æ¸…ç†å¤±æ•ˆçš„ç¼“å­˜
                    del self._vector_store_cache[cache_key]
                    # åˆ›å»ºæ–°å®ä¾‹
                    logger.info(f"åˆ›å»ºæ–°çš„å‘é‡å­˜å‚¨å®ä¾‹: {cache_key}")
                    self._vector_store = self._create_vector_store()
                    self._vector_store_cache[cache_key] = self._vector_store
            else:
                logger.info(f"åˆ›å»ºæ–°çš„å‘é‡å­˜å‚¨å®ä¾‹: {cache_key}")
                self._vector_store = self._create_vector_store()
                self._vector_store_cache[cache_key] = self._vector_store

        return self._vector_store

    @classmethod
    def clear_cache(cls, knowledge_base_id=None):
        """æ¸…ç†å‘é‡å­˜å‚¨ç¼“å­˜"""
        if knowledge_base_id:
            # æ¸…ç†ç‰¹å®šçŸ¥è¯†åº“çš„ç¼“å­˜
            cache_key = str(knowledge_base_id)
            if cache_key in cls._vector_store_cache:
                del cls._vector_store_cache[cache_key]
                logger.info(f"å·²æ¸…ç†çŸ¥è¯†åº“ {cache_key} çš„å‘é‡å­˜å‚¨ç¼“å­˜")

            # åŒæ—¶æ¸…ç†ChromaDBæŒä¹…åŒ–ç›®å½•
            try:
                import shutil
                persist_directory = os.path.join(
                    settings.MEDIA_ROOT,
                    'knowledge_bases',
                    str(knowledge_base_id),
                    'chroma_db'
                )
                if os.path.exists(persist_directory):
                    shutil.rmtree(persist_directory)
                    logger.info(f"å·²æ¸…ç†çŸ¥è¯†åº“ {knowledge_base_id} çš„ChromaDBæŒä¹…åŒ–æ•°æ®")
            except Exception as e:
                logger.warning(f"æ¸…ç†ChromaDBæŒä¹…åŒ–æ•°æ®å¤±è´¥: {e}")
        else:
            # æ¸…ç†æ‰€æœ‰ç¼“å­˜
            cls._vector_store_cache.clear()
            cls._embeddings_cache.clear()
            logger.info("å·²æ¸…ç†æ‰€æœ‰å‘é‡å­˜å‚¨ç¼“å­˜")

    def _create_vector_store(self):
        """åˆ›å»ºChromaDBå‘é‡å­˜å‚¨"""
        persist_directory = os.path.join(
            settings.MEDIA_ROOT,
            'knowledge_bases',
            str(self.knowledge_base.id),
            'chroma_db'
        )

        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(persist_directory, exist_ok=True)

        # åˆ›å»ºChromaDBå®ä¾‹
        chroma_instance = Chroma(
            persist_directory=persist_directory,
            embedding_function=self.embeddings,
            collection_name=f"kb_{self.knowledge_base.id}"
        )

        return chroma_instance

    def _fix_permissions_if_needed(self, persist_directory: str) -> bool:
        """æ£€æµ‹å¹¶ä¿®å¤æƒé™é—®é¢˜ï¼Œè¿”å›æ˜¯å¦æ‰§è¡Œäº†ä¿®å¤"""
        import glob
        fixed = False

        try:
            # ä¿®å¤ç›®å½•æƒé™
            if os.path.exists(persist_directory):
                current_mode = os.stat(persist_directory).st_mode & 0o777
                if current_mode < 0o775:
                    os.chmod(persist_directory, 0o775)
                    logger.info(f"ä¿®å¤ç›®å½•æƒé™: {persist_directory}")
                    fixed = True

            # ä¿®å¤ SQLite æ–‡ä»¶æƒé™
            for pattern in ['*.sqlite3', '*.sqlite3-wal', '*.sqlite3-shm']:
                for filepath in glob.glob(os.path.join(persist_directory, pattern)):
                    current_mode = os.stat(filepath).st_mode & 0o777
                    if current_mode < 0o664:
                        os.chmod(filepath, 0o664)
                        logger.info(f"ä¿®å¤æ–‡ä»¶æƒé™: {filepath}")
                        fixed = True
        except Exception as e:
            logger.warning(f"ä¿®å¤æƒé™å¤±è´¥: {e}")

        return fixed

    def _is_permission_error(self, error: Exception) -> bool:
        """åˆ¤æ–­æ˜¯å¦æ˜¯æƒé™ç›¸å…³é”™è¯¯"""
        error_str = str(error).lower()
        permission_keywords = ['permission', 'access', 'denied', 'readonly', 'locked', 'sqlite']
        return any(keyword in error_str for keyword in permission_keywords)

    def add_documents(self, documents: List[LangChainDocument], document_obj: Document) -> List[str]:
        """æ·»åŠ æ–‡æ¡£åˆ°å‘é‡å­˜å‚¨"""
        try:
            # æ–‡æ¡£åˆ†å—
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=self.knowledge_base.chunk_size,
                chunk_overlap=self.knowledge_base.chunk_overlap
            )
            chunks = text_splitter.split_documents(documents)

            persist_directory = os.path.join(
                settings.MEDIA_ROOT,
                'knowledge_bases',
                str(self.knowledge_base.id),
                'chroma_db'
            )

            try:
                # æ·»åŠ åˆ°å‘é‡å­˜å‚¨
                vector_ids = self.vector_store.add_documents(chunks)
            except Exception as e:
                # å¦‚æœæ˜¯æƒé™é”™è¯¯ï¼Œå°è¯•ä¿®å¤åé‡è¯•
                if self._is_permission_error(e):
                    logger.warning(f"å‘é‡å­˜å‚¨æ“ä½œå¤±è´¥ï¼Œå°è¯•ä¿®å¤æƒé™: {e}")
                    if self._fix_permissions_if_needed(persist_directory):
                        vector_ids = self.vector_store.add_documents(chunks)
                    else:
                        raise
                else:
                    raise

            # ä¿å­˜åˆ†å—ä¿¡æ¯åˆ°æ•°æ®åº“
            self._save_chunks_to_db(chunks, vector_ids, document_obj)

            return vector_ids
        except Exception as e:
            logger.error(f"æ·»åŠ æ–‡æ¡£åˆ°å‘é‡å­˜å‚¨å¤±è´¥: {e}")
            raise

    def _save_chunks_to_db(self, chunks: List[LangChainDocument], vector_ids: List[str], document_obj: Document):
        """ä¿å­˜åˆ†å—ä¿¡æ¯åˆ°æ•°æ®åº“"""
        chunk_objects = []
        for i, (chunk, vector_id) in enumerate(zip(chunks, vector_ids)):
            # è®¡ç®—å†…å®¹å“ˆå¸Œ
            content_hash = hashlib.md5(chunk.page_content.encode()).hexdigest()

            chunk_obj = DocumentChunk(
                document=document_obj,
                chunk_index=i,
                content=chunk.page_content,
                vector_id=vector_id,
                embedding_hash=content_hash,
                start_index=chunk.metadata.get('start_index'),
                end_index=chunk.metadata.get('end_index'),
                page_number=chunk.metadata.get('page')
            )
            chunk_objects.append(chunk_obj)

        DocumentChunk.objects.bulk_create(chunk_objects)

    def similarity_search(self, query: str, k: int = 5, score_threshold: float = 0.1) -> List[Dict[str, Any]]:
        """ç›¸ä¼¼åº¦æœç´¢ï¼ˆå¸¦è‡ªåŠ¨æ¢å¤æœºåˆ¶ï¼‰"""
        # è®°å½•æœç´¢å¼€å§‹ä¿¡æ¯
        embedding_type = type(self.embeddings).__name__
        logger.info(f"ğŸ” å¼€å§‹ç›¸ä¼¼åº¦æœç´¢:")
        logger.info(f"   ğŸ“ æŸ¥è¯¢: '{query}'")
        logger.info(f"   ğŸ¤– ä½¿ç”¨åµŒå…¥æ¨¡å‹: {embedding_type}")
        logger.info(f"   ğŸ¯ è¿”å›æ•°é‡: {k}, ç›¸ä¼¼åº¦é˜ˆå€¼: {score_threshold}")

        persist_directory = os.path.join(
            settings.MEDIA_ROOT,
            'knowledge_bases',
            str(self.knowledge_base.id),
            'chroma_db'
        )

        # å°è¯•æ‰§è¡Œæœç´¢,å¸¦è‡ªåŠ¨æ¢å¤
        max_retries = 2
        for attempt in range(max_retries):
            try:
                # æ‰§è¡Œç›¸ä¼¼åº¦æœç´¢
                results = self.vector_store.similarity_search_with_score(query, k=k)
                break  # æˆåŠŸåˆ™è·³å‡ºå¾ªç¯
            except Exception as e:
                error_msg = str(e)
                # å¤„ç†æƒé™é”™è¯¯
                if self._is_permission_error(e) and attempt < max_retries - 1:
                    logger.warning(f"æœç´¢æ—¶é‡åˆ°æƒé™é—®é¢˜ï¼Œå°è¯•ä¿®å¤: {e}")
                    if self._fix_permissions_if_needed(persist_directory):
                        continue
                # å¤„ç† Collection ä¸å­˜åœ¨é”™è¯¯
                elif ("does not exist" in error_msg or "Collection" in error_msg) and attempt < max_retries - 1:
                    logger.error(f"Collection ä¸å­˜åœ¨ (å°è¯• {attempt + 1}/{max_retries}): {e}")
                    # æ¸…ç†ç¼“å­˜å¹¶é‡è¯•
                    cache_key = str(self.knowledge_base.id)
                    self._vector_store_cache.pop(cache_key, None)
                    self._vector_store = None
                    logger.info("å·²æ¸…ç†ç¼“å­˜,æ­£åœ¨é‡æ–°åˆ›å»º Collection...")
                    continue
                elif "does not exist" in error_msg or "Collection" in error_msg:
                    # æœ€åä¸€æ¬¡å°è¯•ä¹Ÿå¤±è´¥,ç»™å‡ºæ˜ç¡®é”™è¯¯
                    raise ValueError(
                        f"çŸ¥è¯†åº“ '{self.knowledge_base.name}' çš„å‘é‡ç´¢å¼•å·²æŸåã€‚"
                        f"è¯·è”ç³»ç®¡ç†å‘˜é‡å»ºçŸ¥è¯†åº“ç´¢å¼•ã€‚çŸ¥è¯†åº“ID: {self.knowledge_base.id}"
                    )
                else:
                    # å…¶ä»–ç±»å‹çš„é”™è¯¯ç›´æ¥æŠ›å‡º
                    raise

        try:

            logger.debug(f"åŸå§‹æœç´¢ç»“æœæ•°é‡: {len(results)}")
            for i, (doc, score) in enumerate(results):
                logger.debug(f"ç»“æœ {i+1}: åŸå§‹ç›¸ä¼¼åº¦={score:.4f}, å†…å®¹={doc.page_content[:50]}...")

            # å¤„ç†ç›¸ä¼¼åº¦åˆ†æ•°
            processed_results = []
            for doc, score in results:
                # å¯¹äºä¸åŒçš„å‘é‡å­˜å‚¨å’ŒåµŒå…¥æ¨¡å‹ï¼Œç›¸ä¼¼åº¦åˆ†æ•°çš„å«ä¹‰ä¸åŒ
                processed_score = self._process_similarity_score(score)
                processed_results.append((doc, processed_score))
                logger.debug(f"å¤„ç†åç›¸ä¼¼åº¦: {score:.4f} -> {processed_score:.4f}")

            # ç›¸ä¼¼åº¦è¿‡æ»¤
            if processed_results:
                filtered_results = [
                    (doc, score) for doc, score in processed_results
                    if score >= score_threshold
                ]

                # å¦‚æœæ²¡æœ‰ç»“æœä¸”é˜ˆå€¼è¾ƒé«˜ï¼Œé™ä½é˜ˆå€¼é‡è¯•
                if not filtered_results and score_threshold > 0.1:
                    logger.info(f"é˜ˆå€¼ {score_threshold} è¿‡é«˜ï¼Œé™ä½åˆ° 0.1 é‡è¯•")
                    score_threshold = 0.1
                    filtered_results = [
                        (doc, score) for doc, score in processed_results
                        if score >= score_threshold
                    ]

                # å¦‚æœä»ç„¶æ²¡æœ‰ç»“æœï¼Œè¿”å›å¾—åˆ†æœ€é«˜çš„ç»“æœ
                if not filtered_results:
                    logger.info("æ²¡æœ‰ç»“æœé€šè¿‡é˜ˆå€¼è¿‡æ»¤ï¼Œè¿”å›å¾—åˆ†æœ€é«˜çš„ç»“æœ")
                    # æŒ‰ç›¸ä¼¼åº¦æ’åºï¼Œè¿”å›å‰kä¸ª
                    sorted_results = sorted(processed_results, key=lambda x: x[1], reverse=True)
                    filtered_results = sorted_results[:min(k, len(sorted_results))]
            else:
                filtered_results = []

            logger.info(f"ğŸ“Š æœç´¢ç»“æœç»Ÿè®¡:")
            logger.info(f"   ğŸ”¢ åŸå§‹ç»“æœæ•°é‡: {len(results)}")
            logger.info(f"   âœ… è¿‡æ»¤åç»“æœæ•°é‡: {len(filtered_results)}")
            logger.info(f"   ğŸ¯ ä½¿ç”¨çš„é˜ˆå€¼: {score_threshold}")

            # æ ¼å¼åŒ–ç»“æœ
            formatted_results = []
            for i, (doc, score) in enumerate(filtered_results):
                result = {
                    'content': doc.page_content,
                    'metadata': doc.metadata,
                    'similarity_score': float(score)
                }
                formatted_results.append(result)

                # è®°å½•æ¯ä¸ªç»“æœçš„è¯¦ç»†ä¿¡æ¯
                source = doc.metadata.get('source', 'æœªçŸ¥æ¥æº')
                percentage = score * 100
                logger.info(f"   ğŸ“„ ç»“æœ{i+1}: ç›¸ä¼¼åº¦={score:.4f} ({percentage:.1f}%), æ¥æº={source}")

            return formatted_results
        except Exception as e:
            logger.error(f"ç›¸ä¼¼åº¦æœç´¢å¤±è´¥: {e}")
            raise

    def _process_similarity_score(self, raw_score: float) -> float:
        """å¤„ç†ç›¸ä¼¼åº¦åˆ†æ•°ï¼Œç¡®ä¿åˆ†æ•°æœ‰æ„ä¹‰"""
        try:
            # ChromaDB ä½¿ç”¨è·ç¦»åº¦é‡ï¼Œéœ€è¦è½¬æ¢ä¸ºç›¸ä¼¼åº¦
            # å¯¹äºä½™å¼¦è·ç¦»ï¼šç›¸ä¼¼åº¦ = 1 - è·ç¦»
            # å¯¹äºæ¬§å‡ é‡Œå¾—è·ç¦»ï¼šç›¸ä¼¼åº¦ = 1 / (1 + è·ç¦»)

            if raw_score == 0.0:
                # 0è·ç¦»è¡¨ç¤ºå®Œå…¨åŒ¹é…
                return 1.0

            # ChromaDB é»˜è®¤ä½¿ç”¨ä½™å¼¦è·ç¦»ï¼ŒèŒƒå›´æ˜¯ [0, 2]
            # è½¬æ¢ä¸ºç›¸ä¼¼åº¦ï¼šç›¸ä¼¼åº¦ = 1 - (è·ç¦» / 2)
            if raw_score <= 2.0:
                similarity = 1.0 - (raw_score / 2.0)
                return max(0.0, min(1.0, similarity))  # ç¡®ä¿åœ¨ [0, 1] èŒƒå›´å†…
            else:
                # å¦‚æœè·ç¦»å¤§äº2ï¼Œå¯èƒ½æ˜¯æ¬§å‡ é‡Œå¾—è·ç¦»
                # ä½¿ç”¨ 1 / (1 + è·ç¦») å…¬å¼
                similarity = 1.0 / (1.0 + raw_score)
                return similarity

        except Exception as e:
            logger.warning(f"å¤„ç†ç›¸ä¼¼åº¦åˆ†æ•°å¤±è´¥: {e}, åŸå§‹åˆ†æ•°: {raw_score}")
            return max(0.0, min(1.0, raw_score))  # è¿”å›åŸå§‹åˆ†æ•°ï¼Œç¡®ä¿åœ¨åˆç†èŒƒå›´å†…

    def delete_document(self, document: Document):
        """ä»å‘é‡å­˜å‚¨ä¸­åˆ é™¤æ–‡æ¡£"""
        try:
            # è·å–æ–‡æ¡£çš„æ‰€æœ‰åˆ†å—
            chunks = document.chunks.all()
            vector_ids = [chunk.vector_id for chunk in chunks if chunk.vector_id]

            # ä»å‘é‡å­˜å‚¨ä¸­åˆ é™¤
            if vector_ids:
                self.vector_store.delete(vector_ids)

            # ä»æ•°æ®åº“ä¸­åˆ é™¤åˆ†å—è®°å½•
            chunks.delete()
        except Exception as e:
            logger.error(f"åˆ é™¤æ–‡æ¡£å‘é‡å¤±è´¥: {e}")
            raise


class KnowledgeBaseService:
    """çŸ¥è¯†åº“æœåŠ¡"""

    def __init__(self, knowledge_base: KnowledgeBase):
        self.knowledge_base = knowledge_base
        self.document_processor = DocumentProcessor()
        self.vector_manager = VectorStoreManager(knowledge_base)

    def process_document(self, document: Document) -> bool:
        """å¤„ç†æ–‡æ¡£"""
        try:
            # æ›´æ–°çŠ¶æ€ä¸ºå¤„ç†ä¸­
            document.status = 'processing'
            document.save()

            # æ¸…ç†å·²å­˜åœ¨çš„åˆ†å—ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
            document.chunks.all().delete()

            # åŠ è½½æ–‡æ¡£
            langchain_docs = self.document_processor.load_document(document)

            # è®¡ç®—æ–‡æ¡£ç»Ÿè®¡ä¿¡æ¯
            total_content = '\n'.join([doc.page_content for doc in langchain_docs])
            document.word_count = len(total_content.split())
            document.page_count = len(langchain_docs)

            # å‘é‡åŒ–å¹¶å­˜å‚¨
            vector_ids = self.vector_manager.add_documents(langchain_docs, document)

            # æ›´æ–°çŠ¶æ€ä¸ºå®Œæˆ
            document.status = 'completed'
            document.processed_at = timezone.now()
            document.error_message = None
            document.save()

            logger.info(f"æ–‡æ¡£å¤„ç†æˆåŠŸ: {document.id}, ç”Ÿæˆ {len(vector_ids)} ä¸ªåˆ†å—")
            return True

        except Exception as e:
            # æ›´æ–°çŠ¶æ€ä¸ºå¤±è´¥
            document.status = 'failed'
            document.error_message = str(e)
            document.save()

            logger.error(f"æ–‡æ¡£å¤„ç†å¤±è´¥: {document.id}, é”™è¯¯: {e}")
            return False

    def query(self, query_text: str, top_k: int = 5, similarity_threshold: float = 0.7,
              user=None) -> Dict[str, Any]:
        """æŸ¥è¯¢çŸ¥è¯†åº“"""
        start_time = time.time()

        try:
            # è®°å½•æŸ¥è¯¢å¼€å§‹ä¿¡æ¯
            embedding_type = type(self.vector_manager.embeddings).__name__
            logger.info(f"ğŸš€ çŸ¥è¯†åº“æŸ¥è¯¢å¼€å§‹:")
            logger.info(f"   ğŸ“š çŸ¥è¯†åº“: {self.knowledge_base.name}")
            logger.info(f"   ğŸ‘¤ ç”¨æˆ·: {user.username if user else 'åŒ¿å'}")
            logger.info(f"   ğŸ¤– åµŒå…¥æ¨¡å‹: {embedding_type}")
            logger.info(f"   ğŸ’¾ å‘é‡å­˜å‚¨: ChromaDB")

            # æ‰§è¡Œæ£€ç´¢
            retrieval_start = time.time()
            search_results = self.vector_manager.similarity_search(
                query_text, k=top_k, score_threshold=similarity_threshold
            )
            retrieval_time = time.time() - retrieval_start

            # ç”Ÿæˆå›ç­”ï¼ˆè¿™é‡Œå¯ä»¥é›†æˆLLMï¼‰
            generation_start = time.time()
            answer = self._generate_answer(query_text, search_results)
            generation_time = time.time() - generation_start

            total_time = time.time() - start_time

            # è®°å½•æŸ¥è¯¢æ—¥å¿—
            self._log_query(
                query_text, answer, search_results,
                retrieval_time, generation_time, total_time, user
            )

            # è®°å½•æŸ¥è¯¢å®Œæˆä¿¡æ¯
            logger.info(f"âœ… çŸ¥è¯†åº“æŸ¥è¯¢å®Œæˆ:")
            logger.info(f"   â±ï¸  æ£€ç´¢è€—æ—¶: {retrieval_time:.3f}s")
            logger.info(f"   ğŸ¤– ç”Ÿæˆè€—æ—¶: {generation_time:.3f}s")
            logger.info(f"   ğŸ• æ€»è€—æ—¶: {total_time:.3f}s")
            logger.info(f"   ğŸ“Š è¿”å›ç»“æœæ•°: {len(search_results)}")

            return {
                'query': query_text,
                'answer': answer,
                'sources': search_results,
                'retrieval_time': retrieval_time,
                'generation_time': generation_time,
                'total_time': total_time
            }

        except Exception as e:
            logger.error(f"çŸ¥è¯†åº“æŸ¥è¯¢å¤±è´¥: {e}")
            raise

    def _generate_answer(self, query: str, sources: List[Dict[str, Any]]) -> str:
        """ç”Ÿæˆå›ç­”ï¼ˆç®€å•ç‰ˆæœ¬ï¼Œåç»­å¯é›†æˆLLMï¼‰"""
        if not sources:
            return "æŠ±æ­‰ï¼Œæ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚"

        # ç®€å•çš„åŸºäºæ£€ç´¢ç»“æœçš„å›ç­”ç”Ÿæˆ
        context = "\n\n".join([source['content'] for source in sources[:3]])
        return f"åŸºäºæŸ¥è¯¢ã€Œ{query}ã€æ£€ç´¢åˆ°çš„ç›¸å…³å†…å®¹ï¼š\n\n{context}"

    def _log_query(self, query: str, answer: str, sources: List[Dict[str, Any]],
                   retrieval_time: float, generation_time: float, total_time: float, user):
        """è®°å½•æŸ¥è¯¢æ—¥å¿—"""
        try:
            QueryLog.objects.create(
                knowledge_base=self.knowledge_base,
                user=user,
                query=query,
                response=answer,
                retrieved_chunks=[{
                    'content': source['content'][:200] + '...' if len(source['content']) > 200 else source['content'],
                    'metadata': source['metadata'],
                    'score': source['similarity_score']
                } for source in sources],
                similarity_scores=[source['similarity_score'] for source in sources],
                retrieval_time=retrieval_time,
                generation_time=generation_time,
                total_time=total_time
            )
        except Exception as e:
            logger.error(f"è®°å½•æŸ¥è¯¢æ—¥å¿—å¤±è´¥: {e}")

    def delete_document(self, document: Document):
        """åˆ é™¤æ–‡æ¡£"""
        try:
            # ä»å‘é‡å­˜å‚¨ä¸­åˆ é™¤
            self.vector_manager.delete_document(document)

            # åˆ é™¤æ–‡ä»¶
            if document.file:
                if os.path.exists(document.file.path):
                    os.remove(document.file.path)

            # åˆ é™¤æ•°æ®åº“è®°å½•
            document.delete()

            # æ¸…ç†å‘é‡å­˜å‚¨ç¼“å­˜ï¼ˆå› ä¸ºå†…å®¹å·²å˜åŒ–ï¼‰
            VectorStoreManager.clear_cache(self.knowledge_base.id)

            logger.info(f"æ–‡æ¡£åˆ é™¤æˆåŠŸ: {document.id}")

        except Exception as e:
            logger.error(f"åˆ é™¤æ–‡æ¡£å¤±è´¥: {e}")
            raise
